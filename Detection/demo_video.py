import cv2
import torch
import numpy as np
import time
import os
from typing import Tuple

# ============================================================
# å¯¼å…¥ä½ çš„é¡¹ç›®æ¨¡å—
# ç¡®ä¿ demo_video.py å’Œ train.py, model.py åœ¨åŒä¸€ç›®å½•
# ============================================================
from LAGNetwork.model import GraspNetLAG
from LAGNetwork.config import TrainConfig
# æˆ‘ä»¬ç›´æ¥å¤ç”¨ train.py ä¸­çš„è§£ç å‡½æ•°
from LAGNetwork.train import decode_maps_to_boxes_norm


def preprocess_frame(frame: np.ndarray, input_size: int, device: torch.device) -> Tuple[torch.Tensor, float, float]:
    """
    é¢„å¤„ç†è§†é¢‘å¸§ï¼šç¼©æ”¾ -> å½’ä¸€åŒ– -> è½¬ Tensor
    """
    # 1. ç¼©æ”¾å›¾ç‰‡åˆ°æ¨¡å‹è®­ç»ƒæ—¶çš„å¤§å°
    frame_resized = cv2.resize(frame, (input_size, input_size))

    # 2. å½’ä¸€åŒ– [0, 255] -> [0.0, 1.0]
    img = frame_resized.astype(np.float32) / 255.0

    # 3. HWC (OpenCV) -> CHW (PyTorch)
    img = np.transpose(img, (2, 0, 1))

    # 4. å¢åŠ  Batch ç»´åº¦ -> (1, 3, H, W)
    tensor = torch.from_numpy(img).unsqueeze(0).to(device)

    return tensor


def draw_grasps(frame: np.ndarray, boxes_norm: torch.Tensor, color=(0, 255, 0), thickness=2):
    """
    åœ¨åŸå§‹å¸§ä¸Šç»˜åˆ¶æŠ“å–æ¡†
    """
    h_img, w_img = frame.shape[:2]

    if boxes_norm.numel() == 0:
        return frame

    # è½¬ä¸º numpy
    boxes_np = boxes_norm.detach().cpu().numpy()

    for box in boxes_np:
        # box æ˜¯ 8ä¸ªæµ®ç‚¹æ•° [x0, y0, x1, y1, x2, y2, x3, y3]
        pts = box.reshape(4, 2)

        # å°†å½’ä¸€åŒ–åæ ‡ [0,1] è¿˜åŸå›åŸå§‹å›¾ç‰‡å°ºå¯¸
        pts[:, 0] *= (w_img - 1)
        pts[:, 1] *= (h_img - 1)

        # è½¬ä¸º int ä¾› cv2 ä½¿ç”¨
        pts_int = pts.astype(np.int32)

        # ç»˜åˆ¶å¤šè¾¹å½¢ (é—­åˆ)
        cv2.polylines(frame, [pts_int], isClosed=True, color=color, thickness=thickness)

        # ç”»ä¸€ä¸ªçº¢ç‚¹è¡¨ç¤ºâ€œå¤´éƒ¨â€ï¼Œæ–¹ä¾¿çœ‹æ–¹å‘
        cv2.circle(frame, tuple(pts_int[0]), 4, (0, 0, 255), -1)

    return frame


def main():
    # ============================================================
    # ğŸ‘‡ğŸ‘‡ğŸ‘‡ åœ¨è¿™é‡Œä¿®æ”¹è·¯å¾„å’Œå‚æ•° ğŸ‘‡ğŸ‘‡ğŸ‘‡
    # ============================================================

    # 1. æ¨¡å‹è·¯å¾„ (.pt æ–‡ä»¶)
    ckpt_path = "/home/wangzhe/ICME2026/ckpt_lag/b/best.pt"

    # 2. è§†é¢‘æº (0 ä»£è¡¨æ‘„åƒå¤´ï¼Œæˆ–è€…å¡«è§†é¢‘è·¯å¾„ "test.mp4")
    video_source = "/home/wangzhe/ICME2026/MyDataset/Video/b2.avi"

    # 3. å…¶ä»–å‚æ•°
    conf_thresh = 0.15  # ç½®ä¿¡åº¦é˜ˆå€¼
    input_size = 320  # å¿…é¡»å’Œè®­ç»ƒæ—¶çš„ image_size ä¸€è‡´
    top_k = 1  # ç”»é¢ä¸Šæœ€å¤šæ˜¾ç¤ºå‡ ä¸ªæŠ“å–æ¡†
    device_name = "cuda" if torch.cuda.is_available() else "cpu"

    # ============================================================
    # ğŸ‘†ğŸ‘†ğŸ‘† ä¿®æ”¹ç»“æŸ ğŸ‘†ğŸ‘†ğŸ‘†
    # ============================================================

    print(f"[Info] è®¾å¤‡: {device_name}")
    device = torch.device(device_name)

    # --- 1. åŠ è½½æ¨¡å‹ ---
    if not os.path.exists(ckpt_path):
        print(f"[Error] æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {ckpt_path}")
        return

    print(f"[Info] æ­£åœ¨åŠ è½½æ¨¡å‹: {ckpt_path} ...")
    try:
        checkpoint = torch.load(ckpt_path, map_location=device)

        base_channels = 32
        if isinstance(checkpoint, dict) and "cfg" in checkpoint:
            base_channels = checkpoint["cfg"].get("base_channels", 32)

        model = GraspNetLAG(in_channels=3, base_channels=base_channels)

        if isinstance(checkpoint, dict) and "model" in checkpoint:
            model.load_state_dict(checkpoint["model"])
        else:
            model.load_state_dict(checkpoint)

    except Exception as e:
        print(f"[Error] æ¨¡å‹åŠ è½½å‡ºé”™: {e}")
        return

    model.to(device)
    model.eval()

    # --- 2. æ‰“å¼€è§†é¢‘ ---
    cap = cv2.VideoCapture(video_source)
    if not cap.isOpened():
        print(f"[Error] æ— æ³•æ‰“å¼€è§†é¢‘æº: {video_source}")
        return

    print(f"\n===ä»¥æ­¤é”®æ§åˆ¶:===")
    print(f"  'v': å¼€å§‹/æš‚åœ æ£€æµ‹")
    print(f"  'q': é€€å‡ºç¨‹åº")

    fps_avg = 0
    detecting = False  # <--- æ–°å¢çŠ¶æ€æ ‡å¿—ï¼šé»˜è®¤ä¸æ£€æµ‹

    with torch.no_grad():
        while True:
            ret, frame = cap.read()
            if not ret:
                print("[Info] è§†é¢‘æ’­æ”¾ç»“æŸ")
                break

            # åªæœ‰å½“ detecting ä¸º True æ—¶ï¼Œæ‰è¿›è¡Œæ¨¡å‹æ¨ç†
            if detecting:
                t_start = time.time()

                # --- é¢„å¤„ç† ---
                input_tensor = preprocess_frame(frame, input_size, device)

                # --- æ¨ç† ---
                pred = model(input_tensor)

                # --- è§£ç  ---
                pred_one = {
                    "quality": pred["quality"][0],
                    "angle": pred["angle"][0],
                    "width": pred["width"][0],
                }

                boxes_norm = decode_maps_to_boxes_norm(
                    pred_one,
                    topk=top_k,
                    q_thresh=conf_thresh,
                    grasp_h_ratio=0.5,
                    width_scale=3.0
                )

                # --- ç»˜å›¾ ---
                frame = draw_grasps(frame, boxes_norm, color=(0, 255, 0), thickness=2)

                # --- FPS ---
                t_end = time.time()
                fps = 1.0 / (t_end - t_start + 1e-6)
                fps_avg = 0.9 * fps_avg + 0.1 * fps

                # æ˜¾ç¤º FPS å’Œ çŠ¶æ€
                status_text = f"RUNNING | FPS: {fps_avg:.1f}"
                color_text = (0, 255, 0)  # ç»¿è‰²
            else:
                # ä¸æ£€æµ‹æ—¶çš„çŠ¶æ€æç¤º
                status_text = "PAUSED (Press 'v' to Start)"
                color_text = (0, 0, 255)  # çº¢è‰²

            # åœ¨å·¦ä¸Šè§’ç»˜åˆ¶çŠ¶æ€æ–‡å­—
            cv2.putText(frame, status_text, (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color_text, 2)

            cv2.imshow("Grasp Detection", frame)

            # --- æŒ‰é”®ç›‘å¬ ---
            key = cv2.waitKey(1) & 0xFF

            if key == ord('q'):
                break
            elif key == ord('v'):
                detecting = not detecting  # åˆ‡æ¢çŠ¶æ€
                print(f"[Info] Detection status: {detecting}")

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()